{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import multiprocessing\n",
    "import difflib\n",
    "import time\n",
    "import gc\n",
    "import category_encoders as ce\n",
    "import itertools\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from scipy import sparse\n",
    "from fastFM import als, sgd\n",
    "from vowpalwabbit.sklearn_vw import VWClassifier, VWRegressor\n",
    "from collections import Counter\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from models_utils_fe import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tf_idf(df_full, dftr):\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    tf.fit(df_full.question1 + df_full.question2)\n",
    "    q1_tfidf = tf.transform(dftr.question1.values)\n",
    "    q2_tfidf = tf.transform(dftr.question2.values)\n",
    "    tr_tfidf = sparse.hstack([q1_tfidf, q2_tfidf])\n",
    "    print('Final shape:', tr_tfidf.shape)\n",
    "    return tr_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Groupings of ['min_pagerank_sp_network_weighted', 'norm_wmd', 'word_match', '1wl_tfidf_l2_euclidean', 'm_vstack_svd_q1_q1_euclidean', '1wl_tfidf_cosine', 'sk_bi_skew_q2vec', 'm_q1_q2_tf_svd0', 'sk_bi_skew_q1vec', 'skew_q2vec', 'trigram_tfidf_cosine', 'sk_uni_skew_q2vec', 'sk_bi_canberra_distance', 'question1_3', 'sk_uni_skew_q1vec', 'sk_uni_kur_q2vec', 'min_eigenvector_centrality_np_network_weighted', 'avg_world_len2', 'z_word_match', 'sk_uni_kur_q1vec', 'skew_doc2vec_pretrained_lemmat'] columns done.\n",
      "Groupings of ['min_pagerank_sp_network_weighted', 'norm_wmd', 'word_match', '1wl_tfidf_l2_euclidean', 'm_vstack_svd_q1_q1_euclidean', '1wl_tfidf_cosine', 'sk_bi_skew_q2vec', 'm_q1_q2_tf_svd0', 'sk_bi_skew_q1vec', 'skew_q2vec', 'trigram_tfidf_cosine', 'sk_uni_skew_q2vec', 'sk_bi_canberra_distance', 'question1_3', 'sk_uni_skew_q1vec', 'sk_uni_kur_q2vec', 'min_eigenvector_centrality_np_network_weighted', 'avg_world_len2', 'z_word_match', 'sk_uni_kur_q1vec', 'skew_doc2vec_pretrained_lemmat'] columns done.\n",
      "Interactions on ['min_pagerank_sp_network_weighted', 'norm_wmd', 'word_match', '1wl_tfidf_l2_euclidean', 'm_vstack_svd_q1_q1_euclidean'] columns done.\n",
      "Dropping duplicate columns: []\n",
      "Final shape: (404290, 71)\n",
      "Final training data shape: (404290, 614)\n",
      "Final shape: (404290, 537640)\n"
     ]
    }
   ],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "feats_src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/features/uncleaned/'\n",
    "\n",
    "wmd = pd.read_csv(src + 'train_WMD_cleaned_stemmed.csv')\n",
    "wmd = wmd.astype('float32')\n",
    "wmd.replace(np.inf, 1000, inplace = True)\n",
    "skip_thought = pd.read_csv(src + 'train_skipthoughts_Alex_distances.csv')\n",
    "skip_thought = skip_thought.astype('float32')\n",
    "compression = pd.read_csv(src + 'train_LZMAcompression_distance.csv')\n",
    "compression = compression.astype('float32')\n",
    "edit = pd.read_csv(src + 'train_EDITdistance.csv')\n",
    "edit = edit.astype('float32')\n",
    "moments = pd.read_csv(src + 'train_doc2vec_moments.csv')\n",
    "moments = moments.astype('float32')\n",
    "networks_NER = pd.read_csv(src + 'train_networkfeats_NER.csv')\n",
    "networks_NER = networks_NER.astype('float32')\n",
    "xgb_feats = pd.read_csv(feats_src + '/the_1owl/owl_train.csv')\n",
    "y_train = xgb_feats[['is_duplicate']]\n",
    "\n",
    "X_train = pd.read_pickle('Xtrain_500bestCols.pkl')\n",
    "X_train = pd.concat([X_train, wmd, skip_thought, compression, edit, moments, networks_NER], axis = 1)\n",
    "\n",
    "del xgb_feats, wmd, skip_thought, compression, edit, moments, networks_NER\n",
    "gc.collect()\n",
    "\n",
    "best_cols = [\n",
    "    'min_pagerank_sp_network_weighted',\n",
    "    'norm_wmd',\n",
    "    'word_match',\n",
    "    '1wl_tfidf_l2_euclidean',\n",
    "    'm_vstack_svd_q1_q1_euclidean',\n",
    "    '1wl_tfidf_cosine',\n",
    "    'sk_bi_skew_q2vec',\n",
    "    'm_q1_q2_tf_svd0',\n",
    "    'sk_bi_skew_q1vec',\n",
    "    'skew_q2vec',\n",
    "    'trigram_tfidf_cosine',\n",
    "    'sk_uni_skew_q2vec',\n",
    "    'sk_bi_canberra_distance',\n",
    "    'question1_3',\n",
    "    'sk_uni_skew_q1vec',\n",
    "    'sk_uni_kur_q2vec',\n",
    "    'min_eigenvector_centrality_np_network_weighted',\n",
    "    'avg_world_len2',\n",
    "    'z_word_match',\n",
    "    'sk_uni_kur_q1vec',\n",
    "    'skew_doc2vec_pretrained_lemmat']\n",
    "\n",
    "rescale = False\n",
    "X_bin = bin_numerical(X_train, best_cols, 0.1)\n",
    "X_grouped = group_featbyfeat(X_train, best_cols, 'mean')\n",
    "X_grouped2 = group_featbyfeat(X_train, best_cols, 'sum')\n",
    "X_combinations = feature_combinations(X_train, best_cols[:5])\n",
    "\n",
    "\n",
    "X_additional = pd.concat([X_bin, X_grouped, X_grouped2, X_combinations], axis = 1)\n",
    "X_additional = drop_duplicate_cols(X_additional)\n",
    "if rescale:\n",
    "    colnames = X_additional.columns\n",
    "    X_additional = pd.DataFrame(MinMaxScaler().fit_transform(X_additional))\n",
    "    X_additional.columns = colnames\n",
    "\n",
    "X_train = pd.concat([X_train, X_additional], axis = 1)\n",
    "print('Final training data shape:', X_train.shape)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_train.fillna(-999, inplace = True)\n",
    "X_train.replace(np.inf, 999, inplace = True)\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(sparse.csr_matrix(X_train), y_train.is_duplicate.values,\n",
    "                                            stratify = y_train.is_duplicate.values,\n",
    "                                            test_size = 0.2, random_state = 111)\n",
    "\n",
    "del X_bin, X_grouped, X_grouped2, X_combinations, X_additional\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "\n",
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "dftr2 = pd.read_csv(src + 'df_train_lemmatfullcleanSTEMMED.csv')[['question1', 'question2']]\n",
    "dfte2 = pd.read_csv(src + 'df_test_lemmatfullcleanSTEMMED.csv')[['question1', 'question2']]\n",
    "\n",
    "df_full = pd.concat((dftr2, dfte2))\n",
    "X_traintf = tf_idf(df_full, dftr2)\n",
    "\n",
    "X_trtf, X_valtf, y_trtf, y_valtf = train_test_split(X_traintf, y_train.is_duplicate.values,\n",
    "                                            stratify = y_train.is_duplicate.values,\n",
    "                                            test_size = 0.2, random_state = 111)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.save('y_tr', y_tr)\n",
    "np.save('y_val', y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# XGB Feats\n",
    "\n",
    "t = time.time()\n",
    "vw = VWRegressor()\n",
    "vw.fit(X_tr, y_tr)\n",
    "\n",
    "train_pred = vw.predict(X_train)\n",
    "val_pred = vw.predict(X_val)\n",
    "\n",
    "score = vw.score(X_val, y_val)\n",
    "loss = log_loss(y_val, val_pred)\n",
    "print('VW score:', score, 'Logloss score:', loss, '\\n',\n",
    "      'Time it took to train and predict:', time.time() - t)\n",
    "\n",
    "np.save('model_predictions/train_VW_xgbfeats_{}'.format(loss), train_pred)\n",
    "np.save('model_predictions/val_VW_xgbfeats_{}'.format(loss), val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TF-IDF Feats\n",
    "\n",
    "t = time.time()\n",
    "vw = VWRegressor()\n",
    "vw.fit(X_trtf, y_trtf)\n",
    "\n",
    "train_pred = vw.predict(X_trtf)\n",
    "val_pred = vw.predict(X_valtf)\n",
    "\n",
    "score = vw.score(X_valtf, y_valtf)\n",
    "loss = log_loss(y_valtf, val_pred)\n",
    "print('VW score:', score, 'Logloss score:', loss, '\\n',\n",
    "      'Time it took to train and predict:', time.time() - t)\n",
    "\n",
    "np.save('model_predictions/train_VW_tfidffeats_{}'.format(loss), train_pred)\n",
    "np.save('model_predictions/val_VW_tfidffeats_{}'.format(loss), val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Combined feats\n",
    "\n",
    "X_trc = sparse.hstack([X_tr, X_trtf])\n",
    "X_valc = sparse.hstack([X_val, X_valtf])\n",
    "\n",
    "t = time.time()\n",
    "vw = VWRegressor()\n",
    "vw.fit(X_trc, y_tr)\n",
    "\n",
    "train_pred = vw.predict(X_trc)\n",
    "val_pred = vw.predict(X_valc)\n",
    "\n",
    "score = vw.score(X_valc, y_val)\n",
    "loss = log_loss(y_val, val_pred)\n",
    "print('VW score:', score, 'Logloss score:', loss, '\\n',\n",
    "      'Time it took to train and predict:', time.time() - t)\n",
    "\n",
    "np.save('model_predictions/train_VW_combinedfeats_{}'.format(loss), train_pred)\n",
    "np.save('model_predictions/val_VW_combinedfeats_{}'.format(loss), val_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
