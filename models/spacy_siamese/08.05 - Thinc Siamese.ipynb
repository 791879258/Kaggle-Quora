{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import spacy\n",
    "import dill as pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "\n",
    "from thinc.neural.ops import NumpyOps, CupyOps\n",
    "from thinc.neural import Model, ReLu, Softmax, Maxout\n",
    "from thinc.neural import ExtractWindow\n",
    "from thinc.neural.pooling import Pooling, mean_pool, max_pool\n",
    "from thinc.neural._classes.static_vectors import StaticVectors, get_word_ids\n",
    "from thinc.neural._classes.hash_embed import HashEmbed\n",
    "from thinc.neural._classes.embed import Embed\n",
    "from thinc.neural._classes.difference import Siamese, CauchySimilarity\n",
    "from thinc.neural.util import to_categorical\n",
    "from thinc.neural._classes.batchnorm import BatchNorm as BN\n",
    "from thinc.neural._classes.resnet import Residual\n",
    "\n",
    "from thinc.api import layerize, with_flatten, with_getitem, flatten_add_lengths\n",
    "from thinc.api import add, chain, clone, concatenate, Arg\n",
    "\n",
    "from thinc.extra import datasets\n",
    "from thinc.extra.load_nlp import get_spacy, get_vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def track_progress(**context):\n",
    "    '''Print training progress. Called after each epoch.'''\n",
    "    model = context['model']\n",
    "    train_X = context['train_X']\n",
    "    dev_X = context['dev_X']\n",
    "    dev_y = context['dev_y']\n",
    "    n_train = len(train_X)\n",
    "    trainer = context['trainer']\n",
    "    def each_epoch():\n",
    "        global epoch_train_acc, epoch\n",
    "        acc = model.evaluate(dev_X, dev_y)\n",
    "        with model.use_params(trainer.optimizer.averages):\n",
    "            avg_acc = model.evaluate(dev_X, dev_y)\n",
    "        stats = (acc, avg_acc, float(epoch_train_acc) / n_train, trainer.dropout)\n",
    "        print(\"%.3f (%.3f) dev acc, %.3f train acc, %.4f drop\" % stats)\n",
    "        track_stat('dev', epoch, avg_acc)\n",
    "        track_stat('dev_raw', epoch, acc)\n",
    "        track_stat('train', epoch, epoch_train_acc / n_train)\n",
    "        track_stat('batch_size', epoch, trainer.batch_size)\n",
    "        epoch_train_acc = 0.\n",
    "        epoch += 1\n",
    "    return each_epoch\n",
    "\n",
    "def track_stat(name, i, value):\n",
    "    if CTX is None:\n",
    "        return\n",
    "    if name not in CHANNELS:\n",
    "        CHANNELS[name] = CTX.job.create_channel(name, neptune.ChannelType.NUMERIC)\n",
    "    channel = CHANNELS[name]\n",
    "    channel.send(x=i, y=value)\n",
    "\n",
    "def preprocess(ops, nlp, rows, get_ids):\n",
    "    '''Parse the texts with spaCy. Make one-hot vectors for the labels.'''\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for (text1, text2), label in rows:\n",
    "        Xs.append((get_ids([nlp(text1)])[0], get_ids([nlp(text2)])[0]))\n",
    "        ys.append(label)\n",
    "    return Xs, ops.asarray(ys, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_quora_data(src_train, src_test):\n",
    "    df_train = pd.read_csv(src_train)\n",
    "    df_train.dropna(inplace = True)\n",
    "    df_tr, df_val = train_test_split(df_train, test_size = 0.15, random_state = 111)\n",
    "    return df_tr, df_val\n",
    "\n",
    "def train_mine(dataset='quora_mine', width=50, depth=2, min_batch_size=128,\n",
    "        max_batch_size=128, dropout=0.0, dropout_decay=0.0, pooling=\"mean+max\",\n",
    "        nb_epoch=30, pieces=3, L2=0.0, use_gpu=False, out_loc=None, quiet=False,\n",
    "        job_id=None, ws_api_url=None, rest_api_url=None):\n",
    "    global CTX\n",
    "    if job_id is not None:\n",
    "        CTX = neptune.Context()\n",
    "        width = CTX.params.width\n",
    "        L2 = CTX.params.L2\n",
    "        nb_epoch = CTX.params.nb_epoch\n",
    "        depth = CTX.params.depth\n",
    "        max_batch_size = CTX.params.max_batch_size\n",
    "    cfg = dict(locals())\n",
    "\n",
    "    if out_loc:\n",
    "        out_loc = Path(out_loc)\n",
    "        if not out_loc.parent.exists():\n",
    "            raise IOError(\"Can't open output location: %s\" % out_loc)\n",
    "    print(cfg)\n",
    "    if pooling == 'mean+max':\n",
    "        pool_layer = Pooling(mean_pool, max_pool)\n",
    "    elif pooling == \"mean\":\n",
    "        pool_layer = mean_pool\n",
    "    elif pooling == \"max\":\n",
    "        pool_layer = max_pool\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognised pooling\", pooling)\n",
    "\n",
    "\n",
    "    print(\"Load spaCy\")\n",
    "    nlp = get_spacy('en')\n",
    "\n",
    "    if use_gpu:\n",
    "        Model.ops = CupyOps()\n",
    "\n",
    "    print(\"Construct model\")\n",
    "    # Bind operators for the scope of the block:\n",
    "    # * chain (>>): Compose models in a 'feed forward' style,\n",
    "    # i.e. chain(f, g)(x) -> g(f(x))\n",
    "    # * clone (**): Create n copies of a model, and chain them, i.e.\n",
    "    # (f ** 3)(x) -> f''(f'(f(x))), where f, f' and f'' have distinct weights.\n",
    "    # * concatenate (|): Merge the outputs of two models into a single vector,\n",
    "    # i.e. (f|g)(x) -> hstack(f(x), g(x))\n",
    "    Model.lsuv = True\n",
    "    with Model.define_operators({'>>': chain, '**': clone, '|': concatenate,\n",
    "                                 '+': add}):\n",
    "        mwe_encode = ExtractWindow(nW=1) >> Maxout(width, pieces=pieces)\n",
    "\n",
    "        embed = (StaticVectors('en', width)\n",
    "                  + HashEmbed(width, 3000)\n",
    "                  + HashEmbed(width, 3000))\n",
    "        sent2vec = ( # List[spacy.token.Doc]{B}\n",
    "            flatten_add_lengths  # : (ids{T}, lengths{B})\n",
    "            >> with_getitem(0,      # : word_ids{T}\n",
    "                 BN(embed, nO=width)\n",
    "                 >> Residual(mwe_encode ** 2)\n",
    "            ) # : (floats{T, W}, lengths{B})\n",
    "            >> pool_layer\n",
    "            >> Residual(Maxout(width*2, pieces=pieces)**2)\n",
    "        )\n",
    "        model = Siamese(sent2vec, CauchySimilarity(width*2))\n",
    "\n",
    "        \n",
    "    src_train = '../../../features/df_train_spacylemmat_fullclean.csv'\n",
    "    src_test = '../../../features/df_test_spacylemmat_fullclean.csv'\n",
    "    print(\"Read and parse data: %s\" % dataset)\n",
    "    if dataset == 'quora':\n",
    "        train, dev = datasets.quora_questions()\n",
    "    if dataset == 'quora_mine':\n",
    "        train = []\n",
    "        dev = []\n",
    "        dftrain, dfdev = get_quora_data(src_train, src_test)\n",
    "        for i in range(len(dftrain)):\n",
    "            train.append(((dftrain.iloc[i, -3], dftrain.iloc[i, -2]), int(dftrain.iloc[i, 1])))\n",
    "        for i in range(len(dfdev)):\n",
    "            dev.append(((dfdev.iloc[i, -3], dfdev.iloc[i, -2]), int(dfdev.iloc[i, 1])))\n",
    "    elif dataset == 'snli':\n",
    "        train, dev = datasets.snli()\n",
    "    elif dataset == 'stackxc':\n",
    "        train, dev = datasets.stack_exchange()\n",
    "    elif dataset in ('quora+snli', 'snli+quora'):\n",
    "        train, dev = datasets.quora_questions()\n",
    "        train2, dev2 = datasets.snli()\n",
    "        train.extend(train2)\n",
    "        dev.extend(dev2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset: %s\" % dataset)\n",
    "        \n",
    "        \n",
    "    get_ids = get_word_ids(Model.ops)\n",
    "    train_X, train_y = preprocess(model.ops, nlp, train, get_ids)\n",
    "    dev_X, dev_y = preprocess(model.ops, nlp, dev, get_ids)\n",
    "\n",
    "    with model.begin_training(train_X, train_y, **cfg) as (trainer, optimizer):\n",
    "        # Pass a callback to print progress. Give it all the local scope,\n",
    "        # because why not?\n",
    "        trainer.each_epoch.append(track_progress(**locals()))\n",
    "        trainer.batch_size = min_batch_size\n",
    "        batch_size = float(min_batch_size)\n",
    "        print(\"Accuracy before training\", model.evaluate(dev_X, dev_y))\n",
    "        print(\"Train\")\n",
    "        global epoch_train_acc\n",
    "        n_iter = 0\n",
    "        for X, y in trainer.iterate(train_X, train_y, progress_bar=not quiet):\n",
    "            # Slightly useful trick: Decay the dropout as training proceeds.\n",
    "            yh, backprop = model.begin_update(X, drop=trainer.dropout)\n",
    "            assert yh.shape == y.shape, (yh.shape, y.shape)\n",
    "            # No auto-diff: Just get a callback and pass the data through.\n",
    "            # Hardly a hardship, and it means we don't have to create/maintain\n",
    "            # a computational graph. We just use closures.\n",
    "\n",
    "            assert (yh >= 0.).all(), yh\n",
    "            train_acc = ((yh >= 0.5) == (y >= 0.5)).sum()\n",
    "            loss = ((yh-y)**2).sum() / y.shape[0]\n",
    "            track_stat('loss', n_iter, loss)\n",
    "            epoch_train_acc += train_acc\n",
    "            backprop(yh-y, optimizer)\n",
    "            n_iter += 1\n",
    "\n",
    "            # Slightly useful trick: start with low batch size, accelerate.\n",
    "            trainer.batch_size = min(int(batch_size), max_batch_size)\n",
    "            batch_size *= 1.001\n",
    "            track_stat('Batch size', n_iter, trainer.batch_size)\n",
    "        if out_loc:\n",
    "            out_loc = Path(out_loc)\n",
    "            print('Saving to', out_loc)\n",
    "            with out_loc.open('wb') as file_:\n",
    "                pickle.dump(model, file_, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ws_api_url': None, 'job_id': None, 'dropout_decay': 0.0, 'depth': 2, 'dataset': 'quora_mine', 'quiet': False, 'pieces': 3, 'L2': 0.0, 'dropout': 0.0, 'pooling': 'mean+max', 'min_batch_size': 128, 'max_batch_size': 128, 'nb_epoch': 30, 'width': 50, 'out_loc': 'siamese_1sttry', 'use_gpu': False, 'rest_api_url': None}\n",
      "Load spaCy\n",
      "Construct model\n",
      "Read and parse data: quora_mine\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy before training 0.624499084747\n",
      "Train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [10:10, 674.55it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.769 (0.773) dev acc, 0.700 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [10:08, 564.38it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.791 (0.800) dev acc, 0.788 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [10:35, 462.36it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808 (0.812) dev acc, 0.811 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [10:23, 582.61it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.813 (0.818) dev acc, 0.825 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [10:13, 698.02it/s]                             \n",
      "  0%|          | 128/343621 [00:00<05:16, 1085.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.815 (0.824) dev acc, 0.835 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [09:34, 767.92it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823 (0.828) dev acc, 0.843 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [09:50, 581.91it/s]                             \n",
      "  0%|          | 128/343621 [00:00<08:04, 708.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825 (0.831) dev acc, 0.850 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [08:25, 680.27it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 (0.833) dev acc, 0.855 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:56, 965.13it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827 (0.834) dev acc, 0.860 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:51, 978.99it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 (0.835) dev acc, 0.865 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:58, 957.46it/s]                             \n",
      "  0%|          | 128/343621 [00:00<05:06, 1120.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828 (0.835) dev acc, 0.869 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:27, 1050.84it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829 (0.836) dev acc, 0.873 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [06:30, 880.19it/s]                             \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829 (0.836) dev acc, 0.876 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [06:28, 895.27it/s]                             \n",
      "  0%|          | 128/343621 [00:00<05:30, 1038.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829 (0.837) dev acc, 0.879 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:18, 1371.61it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830 (0.837) dev acc, 0.882 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:11, 1102.64it/s]                            \n",
      "  0%|          | 128/343621 [00:00<04:30, 1269.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 (0.837) dev acc, 0.884 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:29, 1042.83it/s]                            \n",
      "  0%|          | 128/343621 [00:00<04:36, 1242.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828 (0.836) dev acc, 0.887 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:29, 1043.37it/s]                            \n",
      "  0%|          | 128/343621 [00:00<05:26, 1051.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828 (0.834) dev acc, 0.888 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [05:59, 960.55it/s]                             \n",
      "  0%|          | 128/343621 [00:00<04:36, 1240.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.829 (0.831) dev acc, 0.891 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:12, 1455.04it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.830 (0.827) dev acc, 0.892 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:13, 1433.68it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825 (0.824) dev acc, 0.894 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:14, 1352.93it/s]                            \n",
      "  0%|          | 128/343621 [00:00<04:37, 1237.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.828 (0.819) dev acc, 0.895 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:13, 1421.50it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 (0.816) dev acc, 0.897 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:15, 1342.44it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 (0.811) dev acc, 0.897 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:15, 1339.22it/s]                            \n",
      "  0%|          | 128/343621 [00:00<04:28, 1278.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.826 (0.806) dev acc, 0.898 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:13, 1354.20it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827 (0.801) dev acc, 0.899 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:14, 1348.03it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825 (0.794) dev acc, 0.899 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:14, 1412.51it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823 (0.792) dev acc, 0.899 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:09, 1379.70it/s]                            \n",
      "  0%|          | 0/343621 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.823 (0.791) dev acc, 0.899 train acc, 0.0000 drop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "343680it [04:14, 1349.09it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.825 (0.792) dev acc, 0.899 train acc, 0.0000 drop\n",
      "Saving to siamese_1sttry\n"
     ]
    }
   ],
   "source": [
    "CTX = None\n",
    "CHANNELS = {}\n",
    "epoch_train_acc = 0.\n",
    "epoch = 0\n",
    "\n",
    "train_mine(use_gpu = False, out_loc = 'siamese_1sttry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
