{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import unicode_literals, print_function\n",
    "import plac\n",
    "import spacy\n",
    "from pathlib import Path\n",
    "import dill as pickle\n",
    "import numpy\n",
    "\n",
    "from thinc.neural import Model, ReLu, Softmax, Maxout\n",
    "from thinc.neural import ExtractWindow\n",
    "from thinc.neural.pooling import Pooling, mean_pool, max_pool\n",
    "from thinc.neural._classes.static_vectors import StaticVectors, get_word_ids\n",
    "from thinc.neural._classes.hash_embed import HashEmbed\n",
    "from thinc.neural._classes.embed import Embed\n",
    "from thinc.neural._classes.difference import Siamese, CauchySimilarity\n",
    "from thinc.neural.util import to_categorical\n",
    "from thinc.neural._classes.batchnorm import BatchNorm as BN\n",
    "from thinc.neural._classes.resnet import Residual\n",
    "from thinc.neural.ops import CupyOps\n",
    "\n",
    "from thinc.api import layerize, with_flatten, with_getitem, flatten_add_lengths\n",
    "from thinc.api import add, chain, clone, concatenate, Arg\n",
    "\n",
    "from thinc.extra import datasets\n",
    "from thinc.extra.load_nlp import get_spacy, get_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def track_progress(**context):\n",
    "    '''Print training progress. Called after each epoch.'''\n",
    "    model = context['model']\n",
    "    train_X = context['train_X']\n",
    "    dev_X = context['dev_X']\n",
    "    dev_y = context['dev_y']\n",
    "    n_train = len(train_X)\n",
    "    trainer = context['trainer']\n",
    "    def each_epoch():\n",
    "        global epoch_train_acc, epoch\n",
    "        acc = model.evaluate(dev_X, dev_y)\n",
    "        with model.use_params(trainer.optimizer.averages):\n",
    "            avg_acc = model.evaluate_logloss(dev_X, dev_y)\n",
    "        stats = (acc, avg_acc, float(epoch_train_acc) / n_train, trainer.dropout)\n",
    "        print(\"%.3f (%.3f) dev acc, %.3f train acc, %.4f drop\" % stats)\n",
    "        track_stat('dev', epoch, avg_acc)\n",
    "        track_stat('dev_raw', epoch, acc)\n",
    "        track_stat('train', epoch, epoch_train_acc / n_train)\n",
    "        track_stat('batch_size', epoch, trainer.batch_size)\n",
    "        epoch_train_acc = 0.\n",
    "        epoch += 1\n",
    "    return each_epoch\n",
    "\n",
    "CHANNELS = {}\n",
    "def track_stat(name, i, value):\n",
    "    if CTX is None:\n",
    "        return\n",
    "    if name not in CHANNELS:\n",
    "        CHANNELS[name] = CTX.job.create_channel(name, neptune.ChannelType.NUMERIC)\n",
    "    channel = CHANNELS[name]\n",
    "    channel.send(x=i, y=value)\n",
    "\n",
    "\n",
    "def preprocess(ops, nlp, rows, get_ids):\n",
    "    '''Parse the texts with spaCy. Make one-hot vectors for the labels.'''\n",
    "    Xs = []\n",
    "    ys = []\n",
    "    for (text1, text2), label in rows:\n",
    "        Xs.append((get_ids([nlp(text1)])[0], get_ids([nlp(text2)])[0]))\n",
    "        ys.append(label)\n",
    "    return Xs, ops.asarray(ys, dtype='float32')\n",
    "\n",
    "@layerize\n",
    "def logistic(X, drop=0.):\n",
    "    ops = Model.ops\n",
    "    y = 1. / (1. + ops.xp.exp(-X))\n",
    "    def backward(dy, sgd=None):\n",
    "        return dy * y * (1-y)\n",
    "    return y, backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_quora_data(src_train, src_test):\n",
    "    df_train = pd.read_csv(src_train)\n",
    "    df_train.dropna(inplace = True)\n",
    "    df_tr, df_val = train_test_split(df_train, test_size = 0.15, random_state = 111)\n",
    "    return df_tr, df_val\n",
    "\n",
    "def train_mine(dataset='quora_mine', width=50, depth=2, min_batch_size=128,\n",
    "        max_batch_size=128, dropout=0.0, dropout_decay=0.0, pooling=\"mean+max\",\n",
    "        nb_epoch=30, pieces=3, L2=0.0, use_gpu=False, out_loc=None, quiet=False,\n",
    "        job_id=None, ws_api_url=None, rest_api_url=None):\n",
    "    global CTX\n",
    "    if job_id is not None:\n",
    "        CTX = neptune.Context()\n",
    "        width = CTX.params.width\n",
    "        L2 = CTX.params.L2\n",
    "        nb_epoch = CTX.params.nb_epoch\n",
    "        depth = CTX.params.depth\n",
    "        max_batch_size = CTX.params.max_batch_size\n",
    "    cfg = dict(locals())\n",
    "\n",
    "    if out_loc:\n",
    "        out_loc = Path(out_loc)\n",
    "        if not out_loc.parent.exists():\n",
    "            raise IOError(\"Can't open output location: %s\" % out_loc)\n",
    "    print(cfg)\n",
    "    if pooling == 'mean+max':\n",
    "        pool_layer = Pooling(mean_pool, max_pool)\n",
    "    elif pooling == \"mean\":\n",
    "        pool_layer = mean_pool\n",
    "    elif pooling == \"max\":\n",
    "        pool_layer = max_pool\n",
    "    else:\n",
    "        raise ValueError(\"Unrecognised pooling\", pooling)\n",
    "\n",
    "\n",
    "    print(\"Load spaCy\")\n",
    "    nlp = get_spacy('en')\n",
    "\n",
    "    if use_gpu:\n",
    "        Model.ops = CupyOps()\n",
    "\n",
    "    print(\"Construct model\")\n",
    "    # Bind operators for the scope of the block:\n",
    "    # * chain (>>): Compose models in a 'feed forward' style,\n",
    "    # i.e. chain(f, g)(x) -> g(f(x))\n",
    "    # * clone (**): Create n copies of a model, and chain them, i.e.\n",
    "    # (f ** 3)(x) -> f''(f'(f(x))), where f, f' and f'' have distinct weights.\n",
    "    # * concatenate (|): Merge the outputs of two models into a single vector,\n",
    "    # i.e. (f|g)(x) -> hstack(f(x), g(x))\n",
    "    Model.lsuv = True\n",
    "    #Model.ops = CupyOps()\n",
    "    with Model.define_operators({'>>': chain, '**': clone, '|': concatenate,\n",
    "                                 '+': add}):\n",
    "        mwe_encode = ExtractWindow(nW=1) >> BN(Maxout(width, drop_factor=0.0, pieces=pieces))\n",
    "\n",
    "        sent2vec = ( # List[spacy.token.Doc]{B}\n",
    "            flatten_add_lengths  # : (ids{T}, lengths{B})\n",
    "            >> with_getitem(0,\n",
    "                (StaticVectors('en', width)\n",
    "                   + HashEmbed(width, 3000)\n",
    "                   + HashEmbed(width, 3000))\n",
    "                #>> Residual(mwe_encode ** 2)\n",
    "                ) # : word_ids{T}\n",
    "            >> Pooling(mean_pool, max_pool)\n",
    "            #>> Residual(BN(Maxout(width*2, pieces=pieces), nO=width*2)**2)\n",
    "            >> Maxout(width*2, pieces=pieces, drop_factor=0.0)\n",
    "            >> logistic\n",
    "        )\n",
    "        model = Siamese(sent2vec, CauchySimilarity(width*2))\n",
    "\n",
    "        \n",
    "    src_train = '../../../features/df_train_lemmatfullcleanSTEMMED.csv'\n",
    "    src_test = '../../../features/df_test_lemmatfullcleanSTEMMED.csv'\n",
    "    print(\"Read and parse data: %s\" % dataset)\n",
    "    if dataset == 'quora':\n",
    "        train, dev = datasets.quora_questions()\n",
    "    if dataset == 'quora_mine':\n",
    "        train = []\n",
    "        dev = []\n",
    "        dftrain, dfdev = get_quora_data(src_train, src_test)\n",
    "        for i in range(len(dftrain)):\n",
    "            train.append(((dftrain.iloc[i, -3], dftrain.iloc[i, -2]), int(dftrain.iloc[i, 1])))\n",
    "        for i in range(len(dfdev)):\n",
    "            dev.append(((dfdev.iloc[i, -3], dfdev.iloc[i, -2]), int(dfdev.iloc[i, 1])))\n",
    "    elif dataset == 'snli':\n",
    "        train, dev = datasets.snli()\n",
    "    elif dataset == 'stackxc':\n",
    "        train, dev = datasets.stack_exchange()\n",
    "    elif dataset in ('quora+snli', 'snli+quora'):\n",
    "        train, dev = datasets.quora_questions()\n",
    "        train2, dev2 = datasets.snli()\n",
    "        train.extend(train2)\n",
    "        dev.extend(dev2)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown dataset: %s\" % dataset)\n",
    "        \n",
    "        \n",
    "    get_ids = get_word_ids(Model.ops)\n",
    "    train_X, train_y = preprocess(model.ops, nlp, train, get_ids)\n",
    "    dev_X, dev_y = preprocess(model.ops, nlp, dev, get_ids)\n",
    "\n",
    "    get_ids = get_word_ids(Model.ops)\n",
    "    train_X, train_y = preprocess(model.ops, nlp, train, get_ids)\n",
    "    dev_X, dev_y = preprocess(model.ops, nlp, dev, get_ids)\n",
    "\n",
    "    with model.begin_training(train_X[:10000], train_y[:10000], **cfg) as (trainer, optimizer):\n",
    "        # Pass a callback to print progress. Give it all the local scope,\n",
    "        # because why not?\n",
    "        trainer.each_epoch.append(track_progress(**locals()))\n",
    "        trainer.batch_size = min_batch_size\n",
    "        batch_size = float(min_batch_size)\n",
    "        print(\"Accuracy before training\", model.evaluate_logloss(dev_X, dev_y))\n",
    "        print(\"Train\")\n",
    "        global epoch_train_acc\n",
    "        n_iter = 0\n",
    "\n",
    "        for X, y in trainer.iterate(train_X, train_y, progress_bar=not quiet):\n",
    "            # Slightly useful trick: Decay the dropout as training proceeds.\n",
    "            yh, backprop = model.begin_update(X, drop=trainer.dropout)\n",
    "            assert yh.shape == y.shape, (yh.shape, y.shape)\n",
    "\n",
    "            assert (yh >= 0.).all(), yh\n",
    "            train_acc = ((yh >= 0.5) == (y >= 0.5)).sum()\n",
    "            loss = model.ops.xp.abs(yh-y).mean()\n",
    "            track_stat('loss', n_iter, loss)\n",
    "            track_stat('train acc', n_iter, train_acc)\n",
    "            track_stat('LR', n_iter, optimizer.lr(n_iter+1))\n",
    "            epoch_train_acc += train_acc\n",
    "            backprop(yh-y, optimizer)\n",
    "            optimizer.set_loss(loss)\n",
    "            n_iter += 1\n",
    "\n",
    "            # Slightly useful trick: start with low batch size, accelerate.\n",
    "            trainer.batch_size = min(int(batch_size), max_batch_size)\n",
    "            batch_size *= 1.001\n",
    "            track_stat('Batch size', n_iter, y.shape[0])\n",
    "        if out_loc:\n",
    "            out_loc = Path(out_loc)\n",
    "            print('Saving to', out_loc)\n",
    "            with out_loc.open('wb') as file_:\n",
    "                pickle.dump(model, file_, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'min_batch_size': 128, 'dropout': 0.0, 'pieces': 3, 'L2': 0.0, 'dataset': u'quora_mine', 'rest_api_url': None, 'job_id': None, 'ws_api_url': None, 'use_gpu': False, 'nb_epoch': 30, 'max_batch_size': 128, 'quiet': False, 'pooling': u'mean+max', 'depth': 2, 'dropout_decay': 0.0, 'out_loc': u'siamese_1sttry', 'width': 50}\n",
      "Load spaCy\n",
      "Construct model\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "CauchySimilarity() takes exactly 2 arguments (1 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1673c2455d82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain_mine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muse_gpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_loc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'siamese_1sttry'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-4594e1b9a5e3>\u001b[0m in \u001b[0;36mtrain_mine\u001b[0;34m(dataset, width, depth, min_batch_size, max_batch_size, dropout, dropout_decay, pooling, nb_epoch, pieces, L2, use_gpu, out_loc, quiet, job_id, ws_api_url, rest_api_url)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;34m>>\u001b[0m \u001b[0mlogistic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m         )\n\u001b[0;32m---> 69\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSiamese\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent2vec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCauchySimilarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: CauchySimilarity() takes exactly 2 arguments (1 given)"
     ]
    }
   ],
   "source": [
    "CTX = None\n",
    "CHANNELS = {}\n",
    "epoch_train_acc = 0.\n",
    "epoch = 0\n",
    "\n",
    "train_mine(use_gpu = False, out_loc = 'siamese_1sttry')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
