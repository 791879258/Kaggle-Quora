{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "from __future__ import division, unicode_literals, print_function\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import spacy\n",
    "import plac\n",
    "import ujson as json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import en_core_web_md\n",
    "import en_vectors_glove_md\n",
    "from tqdm import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:\n",
    "    import pickle\n",
    "\n",
    "from spacy_hook import get_embeddings, get_word_ids\n",
    "from spacy_hook import create_similarity_pipeline\n",
    "from decomposable_merge import build_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import InputSpec, Layer, Input, Dense, merge\n",
    "from keras.layers import Lambda, Activation, Dropout, Embedding, TimeDistributed\n",
    "from keras.layers import Bidirectional, GRU, LSTM\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.layers.advanced_activations import ELU\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model, model_from_json\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.layers import Merge\n",
    "from keras.layers.advanced_activations import PReLU\n",
    "from keras.layers.merge import concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_encode(df, settings, shape):\n",
    "    print('Encoding data according to following settings:', settings, '\\n', shape)\n",
    "    train_texts1, train_texts2 = df['question1'], df['question2']\n",
    "    print(\"Loading spaCy\")\n",
    "    nlp = en_core_web_md.load()\n",
    "    assert nlp.path is not None\n",
    "    print(\"Processing texts...\")\n",
    "    encoded_data = []\n",
    "    for texts in tqdm((train_texts1, train_texts2)):\n",
    "        encoded_data.append(get_word_ids(list(nlp.pipe(texts, n_threads=10, batch_size=5000)),\n",
    "                         max_length=shape[0],\n",
    "                         rnn_encode=settings['gru_encode'],\n",
    "                         tree_truncate=settings['tree_truncate']))\n",
    "    q1, q2 = encoded_data\n",
    "    return q1, q2\n",
    "\n",
    "def get_train():\n",
    "    abhishek_feats = pd.read_csv('../../../../data/features/abhishek/train_features.csv',\n",
    "                      encoding = 'ISO-8859-1').iloc[:, 2:]\n",
    "    text_feats = pd.read_csv('../../../../data/features/spacylemmat_fullclean/train_whq_with_jaccard_feats.csv')\n",
    "    eda_feats = pd.read_csv('../../../../data/features/spacylemmat_fullclean/train_eda_features.csv')\n",
    "    mephisto_feats = pd.read_csv('../../../../data/features/spacylemmat_fullclean/train_mephistopeheles_features.csv')\n",
    "    turkewitz_feats = pd.read_csv('../../../../data/features/spacylemmat_fullclean/train_turkewitz_features.csv')\n",
    "    srk_feats = pd.read_csv('../../../../data/features/spacylemmat_fullclean/train_SRKgrams_features.csv')\n",
    "    turkewitz_feats = turkewitz_feats[['q1_freq', 'q2_freq']]\n",
    "\n",
    "    df = pd.concat([mephisto_feats, abhishek_feats, turkewitz_feats], axis = 1)\n",
    "    df2 = pd.concat([eda_feats, text_feats, srk_feats], axis = 1)\n",
    "    df = df.merge(df2, on = 'id', how = 'left')\n",
    "    print('Original shape:', df.shape)\n",
    "    df.fillna(-999, inplace = True)\n",
    "    \n",
    "    y = df['is_duplicate_y']\n",
    "    \n",
    "    dfc = df.iloc[0:1000,:]\n",
    "    dfc = dfc.T.drop_duplicates().T\n",
    "    duplicate_cols = sorted(list(set(df.columns).difference(set(dfc.columns))))\n",
    "    print('Dropping duplicate columns:', duplicate_cols)\n",
    "    df.drop(duplicate_cols, axis = 1, inplace = True)\n",
    "    print('Final shape:', df.shape)\n",
    "    \n",
    "    df.drop(['is_duplicate_x',], axis = 1, inplace = True)\n",
    "    X = df.iloc[:, 6:]\n",
    "    X.drop(['question1_y','question2_y'], axis = 1, inplace = True)\n",
    "    print('Train data loaded.', '\\n', 'Training data shape:', X.shape)\n",
    "    return X, y\n",
    "\n",
    "def create_mergevalidset(data_1, data_2, datafeats, labels):\n",
    "    np.random.seed(1234)\n",
    "    perm = np.random.permutation(len(data_1))\n",
    "    idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "    idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "    \n",
    "    data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "    data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "    labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "    dataf_train = np.vstack((datafeats[idx_train], datafeats[idx_train]))\n",
    "    \n",
    "    data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "    data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "    labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "    dataf_val = np.vstack((datafeats[idx_val], datafeats[idx_val]))\n",
    "    return data_1_train, data_2_train, dataf_train, labels_train, data_1_val, data_2_val, dataf_val, labels_val\n",
    "\n",
    "def create_stratified_split(data_1, data_2, datafeats, labels):\n",
    "    data1_tr, data1_val, y1_tr, y1_val = train_test_split(data_1, labels, stratify = labels,\n",
    "                                                        test_size = 0.2, random_state = 111)\n",
    "    data2_tr, data2_val, y2_tr, y2_val = train_test_split(data_2, labels, stratify = labels,\n",
    "                                                        test_size = 0.2, random_state = 111)\n",
    "    dataf_train, dataf_val, yf_tr, yf_val = train_test_split(datafeats, labels, stratify = labels,\n",
    "                                                        test_size = 0.2, random_state = 111)\n",
    "    return data1_tr, data2_tr, dataf_train, yf_tr, data1_val, data2_val, dataf_val, yf_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original shape: (404290, 119)\n",
      "Dropping duplicate columns: ['common_unigrams_len', 'common_unigrams_ratio', 'is_duplicate_y', 'len_q1', 'len_q2', 'm_q1_q2_tf_svd1', 'qid1_y', 'qid2_y', 'test_id_y']\n",
      "Final shape: (404290, 110)\n",
      "Train data loaded. \n",
      " Training data shape: (404290, 101)\n"
     ]
    }
   ],
   "source": [
    "src_train_raw = '../../../data/train.csv'\n",
    "src_test_raw = '../../../data/test.csv'\n",
    "\n",
    "src_train = '../../../features/df_train_spacylemmat_fullclean.csv'\n",
    "src_test = '../../../features/df_test_spacylemmat_fullclean.csv'\n",
    "\n",
    "\n",
    "q1 = np.load('../../../features/q1train_spacylemmat_fullclean_170len_treetrunc.npy')\n",
    "q2 = np.load('../../../features/q2train_spacylemmat_fullclean_170len_treetrunc.npy')\n",
    "X, y = get_train()\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "ncols = X.shape[1]\n",
    "\n",
    "#y = to_categorical(y)\n",
    "tr_q1, tr_q2, tr_feats, y_tr, val_q1, val_q2, val_feats, y_val = create_stratified_split(q1, q2, X.values, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    'lr': 0.0005,\n",
    "    'dropout': 0.2,\n",
    "    'batch_size': 128,\n",
    "    'nr_epoch': 100,\n",
    "    'tree_truncate': True,\n",
    "    'gru_encode': False,\n",
    "    }\n",
    "\n",
    "max_length = 170\n",
    "nr_hidden = 256\n",
    "ncols = X.shape[1]\n",
    "shape = (max_length, nr_hidden, 2, ncols)\n",
    "\n",
    "re_weight = True\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(get_embeddings(nlp.vocab), shape, settings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 323432 samples, validate on 80858 samples\n",
      "Epoch 1/100\n",
      "323432/323432 [==============================] - 243s - loss: 0.3932 - acc: 0.6972 - val_loss: 0.4804 - val_acc: 0.7645\n",
      "Epoch 2/100\n",
      "323432/323432 [==============================] - 242s - loss: 0.3118 - acc: 0.7515 - val_loss: 0.4248 - val_acc: 0.7940\n",
      "Epoch 3/100\n",
      "323432/323432 [==============================] - 245s - loss: 0.2810 - acc: 0.7830 - val_loss: 0.4085 - val_acc: 0.8121\n",
      "Epoch 4/100\n",
      "323432/323432 [==============================] - 246s - loss: 0.2611 - acc: 0.8047 - val_loss: 0.3886 - val_acc: 0.8263\n",
      "Epoch 5/100\n",
      "323432/323432 [==============================] - 229s - loss: 0.2466 - acc: 0.8199 - val_loss: 0.3969 - val_acc: 0.8271\n",
      "Epoch 6/100\n",
      "323432/323432 [==============================] - 222s - loss: 0.2341 - acc: 0.8326 - val_loss: 0.4036 - val_acc: 0.8315\n",
      "Epoch 7/100\n",
      "323432/323432 [==============================] - 220s - loss: 0.2238 - acc: 0.8420 - val_loss: 0.3965 - val_acc: 0.8343\n",
      "Epoch 8/100\n",
      "323432/323432 [==============================] - 232s - loss: 0.2144 - acc: 0.8498 - val_loss: 0.3873 - val_acc: 0.8373\n",
      "Epoch 9/100\n",
      "323432/323432 [==============================] - 232s - loss: 0.2060 - acc: 0.8570 - val_loss: 0.3837 - val_acc: 0.8438\n",
      "Epoch 10/100\n",
      "323432/323432 [==============================] - 223s - loss: 0.1986 - acc: 0.8645 - val_loss: 0.4000 - val_acc: 0.8326\n",
      "Epoch 11/100\n",
      "323432/323432 [==============================] - 223s - loss: 0.1923 - acc: 0.8693 - val_loss: 0.4145 - val_acc: 0.8400\n",
      "Epoch 12/100\n",
      "118784/323432 [==========>...................] - ETA: 134s - loss: 0.1798 - acc: 0.8796"
     ]
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint('decomposable_merged.h5',\n",
    "                                    monitor='val_loss', \n",
    "                                    verbose = 0, save_best_only = True),\n",
    "                 EarlyStopping(monitor='val_loss', patience = 10, verbose = 1)]\n",
    "\n",
    "model.fit([tr_q1, tr_q2, tr_feats], y_tr,\n",
    "        validation_data=([val_q1, val_q2, val_feats], y_val), class_weight = class_weight,\n",
    "        nb_epoch=settings['nr_epoch'], batch_size=settings['batch_size'], callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
