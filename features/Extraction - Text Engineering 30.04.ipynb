{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "import random\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction import FeatureHasher\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.manifold import LocallyLinearEmbedding, SpectralEmbedding, TSNE\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "\n",
    "from textstat.textstat import textstat\n",
    "from gensim import corpora\n",
    "from gensim.models import KeyedVectors\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import *\n",
    "from nltk import word_tokenize, ngrams\n",
    "from nltk.tag import AffixTagger\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from scipy.spatial import distance\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import PCA, TruncatedSVD, LatentDirichletAllocation\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from scipy.spatial.distance import cosine, cityblock, jaccard, canberra, euclidean, minkowski, braycurtis\n",
    "\n",
    "seed = 1337"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lowercase(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.lower()\n",
    "    return df\n",
    "\n",
    "def unidecode(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.encode('ascii', 'ignore')\n",
    "    return df\n",
    "\n",
    "def remove_nonalpha(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].str.replace('\\W+', ' ')\n",
    "    return df\n",
    "\n",
    "def repair_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (''.join(''.join(s)[:2] for _, s in itertools.groupby(x))))\n",
    "    return df\n",
    "\n",
    "def concat_words(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: (' '.join(i for i in x)))\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def tokenize(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "    return df\n",
    "\n",
    "def ngram(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), n)])\n",
    "    return df\n",
    "\n",
    "def skipgram(df2, ngram_n, skip_n):\n",
    "    def random_sample(words_list, skip_n):\n",
    "        return [words_list[i] for i in sorted(random.sample(range(len(words_list)), skip_n))]\n",
    "    \n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in ngrams(word_tokenize(x), ngram_n)])\n",
    "        df[i] = df[i].apply(lambda x: random_sample(x, skip_n))\n",
    "    return df\n",
    "\n",
    "def chargram(df2, n):\n",
    "    def chargram_generate(string, n):\n",
    "        return [string[i:i+n] for i in range(len(string)-n+1)]\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in chargram_generate(x, 3)])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_stops(df2, stopwords):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    return df\n",
    "\n",
    "def remove_extremes(df2, stopwords, min_count = 3, max_frequency = 0.75):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords])\n",
    "    tokenized = []\n",
    "    for i in text_feats:\n",
    "        tokenized += df[i].tolist()\n",
    "    dictionary = corpora.Dictionary(tokenized)\n",
    "    dictionary.filter_extremes(no_below = min_count, no_above = max_frequency)\n",
    "    dictionary.compactify()\n",
    "    df = df2.copy()\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i for i in word_tokenize(x) if i not in stopwords and i not in \n",
    "                                      list(dictionary.token2id.keys())])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def chop(df2, n):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: [i[:n] for i in word_tokenize(x)])\n",
    "    return df\n",
    "\n",
    "def stem(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([stemmer.stem(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def lemmat(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: ' '.join([lemmatizer.lemmatize(i) for i in word_tokenize(x)]))\n",
    "    return df\n",
    "\n",
    "def extract_entity(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i in text_feats:\n",
    "        df[i] = df[i].apply(lambda x: word_tokenize(x))\n",
    "        df[i] = df[i].apply(lambda x: nltk.pos_tag(x))\n",
    "        df[i] = df[i].apply(lambda x: [i[1:] for i in x])\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def doc_features(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['num_characters_{}'.format(i)] = df[col].map(lambda x: len(str(x))) # length of sentence\n",
    "        df['num_words_{}'.format(i)] = df[col].map(lambda x: len(str(x).split())) # number of words\n",
    "        df['num_spaces_{}'.format(i)] = df[col].map(lambda x: x.count(' '))\n",
    "        df['num_alpha_{}'.format(i)] = df[col].apply(lambda x: sum(i.isalpha()for i in x))\n",
    "        df['num_nonalpha_{}'.format(i)] = df[col].apply(lambda x: sum(1-i.isalpha()for i in x))\n",
    "    return df\n",
    "\n",
    "def get_readability(df2):\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    for i, col in enumerate(text_feats):\n",
    "        df['flesch_reading_ease{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_reading_ease(x))\n",
    "        df['smog_index{}'.format(i)] = df[col].apply(lambda x: textstat.smog_index(x))\n",
    "        df['flesch_kincaid_grade{}'.format(i)] = df[col].apply(lambda x: textstat.flesch_kincaid_grade(x))\n",
    "        df['coleman_liau_index{}'.format(i)] = df[col].apply(lambda x: textstat.coleman_liau_index(x))\n",
    "        df['automated_readability_index{}'.format(i)] = df[col].apply(lambda x: textstat.automated_readability_index(x))\n",
    "        df['dale_chall_readability_score{}'.format(i)] = df[col].apply(lambda x: textstat.dale_chall_readability_score(x))\n",
    "        df['difficult_words{}'.format(i)] = df[col].apply(lambda x: textstat.difficult_words(x))\n",
    "        df['linsear_write_formula{}'.format(i)] = df[col].apply(lambda x: textstat.linsear_write_formula(x))\n",
    "        df['gunning_fog{}'.format(i)] = df[col].apply(lambda x: textstat.gunning_fog(x))\n",
    "        df['text_standard{}'.format(i)] = df[col].apply(lambda x: textstat.text_standard(x))\n",
    "    return df\n",
    "\n",
    "def bag_of_words(df2):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    bow = cv.fit_transform(df.question1 + df.question2)\n",
    "    return bow\n",
    "\n",
    "def tf_idf(df2):\n",
    "    df = df2.copy()\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999)\n",
    "    tfidf = tf.fit_transform(df.question1 + df.question2)\n",
    "    return tfidf\n",
    "\n",
    "def LDA_text2(df2, ntopics):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    lda = LatentDirichletAllocation(ntopics, random_state = seed, n_jobs = 1)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    lda.fit(bow)\n",
    "    ldas = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        ldas.append(lda.transform(bow_i))\n",
    "    return ldas\n",
    "\n",
    "def SVD_text(df2, ndims):\n",
    "    df = df2.copy()\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text(df2, ndims):\n",
    "    cv = CountVectorizer(stop_words = 'english', min_df = 2, max_df = 0.99)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    cv.fit(df.question1 + df.question2)\n",
    "    bow = cv.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = cv.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stemmer = snowball.SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stopwords_eng = stopwords.words('english')\n",
    "words = re.compile(r\"\\w+\",re.I)\n",
    "\n",
    "model = KeyedVectors.load_word2vec_format('/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/data/embeddings/GoogleNews-vectors-negative300.bin',                            \n",
    "                                             binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "src = '/media/w/1c392724-ecf3-4615-8f3c-79368ec36380/DS Projects/Kaggle/Quora/scripts/features/'\n",
    "\n",
    "dftr2 = pd.read_csv('df_train_lemmatfullcleanSTEMMED.csv')\n",
    "dfte2 = pd.read_csv('df_test_lemmatfullcleanSTEMMED.csv')\n",
    "\n",
    "df_full = pd.concat((dftr2, dfte2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/spatial/distance.py:505: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - np.dot(u, v) / (norm(u) * norm(v))\n",
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/spatial/distance.py:616: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  np.double(np.bitwise_or(u != 0, v != 0).sum()))\n",
      "/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/spatial/distance.py:810: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  return abs(u - v).sum() / abs(u + v).sum()\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-6acc8c0a5cc3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mrun_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TFIDF_3grams_words_50dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m \u001b[0mrun_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'TFIDF_5grams_words_50dim'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'word'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-6acc8c0a5cc3>\u001b[0m in \u001b[0;36mrun_transforms\u001b[0;34m(transformation_name, ndims, gram_range, analyze, test)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mrun_transforms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mlsa_dff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLSA_text_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m     \u001b[0msvd_dff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSVD_text_tfidf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_full\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndims\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgram_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyze\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mlsa_tr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlsa_te\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msplit_traintest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlsa_dff\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-6acc8c0a5cc3>\u001b[0m in \u001b[0;36mLSA_text_tfidf\u001b[0;34m(df2, ndims, gram_range, analyze)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mbow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquestion2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0mtransformed_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mnormed_bow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransformed_bow\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0mReturns\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtransformer\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \"\"\"\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/decomposition/truncated_svd.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    171\u001b[0m             U, Sigma, VT = randomized_svd(X, self.n_components,\n\u001b[1;32m    172\u001b[0m                                           \u001b[0mn_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m                                           random_state=random_state)\n\u001b[0m\u001b[1;32m    174\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"unknown algorithm %r\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0malgorithm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_svd\u001b[0;34m(M, n_components, n_oversamples, n_iter, power_iteration_normalizer, transpose, flip_sign, random_state)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     Q = randomized_range_finder(M, n_random, n_iter,\n\u001b[0;32m--> 364\u001b[0;31m                                 power_iteration_normalizer, random_state)\n\u001b[0m\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m     \u001b[0;31m# project M to the (k + p) dimensional space using the basis vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/sklearn/utils/extmath.py\u001b[0m in \u001b[0;36mrandomized_range_finder\u001b[0;34m(A, size, n_iter, power_iteration_normalizer, random_state)\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0mQ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'LU'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m             \u001b[0mQ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlinalg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msafe_sparse_dot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mpower_iteration_normalizer\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'QR'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/w/anaconda3/envs/idp3/lib/python3.5/site-packages/scipy/linalg/decomp_lu.py\u001b[0m in \u001b[0;36mlu\u001b[0;34m(a, permute_l, overwrite_a, check_finite)\u001b[0m\n\u001b[1;32m    183\u001b[0m     \u001b[0moverwrite_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moverwrite_a\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_datacopied\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[0mflu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_flinalg_funcs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'lu'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m     \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpermute_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpermute_l\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moverwrite_a\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverwrite_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m         raise ValueError('illegal value in %d-th argument of '\n",
      "\u001b[0;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def split_traintest(l):\n",
    "    train = []\n",
    "    test = []\n",
    "    for i in l:\n",
    "        train.append(i[:dftr2.shape[0]])\n",
    "        test.append(i[dftr2.shape[0]:])\n",
    "    return train, test\n",
    "\n",
    "def get_distances(transformation_name, question1_vectors, question2_vectors):\n",
    "    data = pd.DataFrame()\n",
    "    data['cosine_distance_{}'.format(transformation_name)] = [cosine(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['cityblock_distance_{}'.format(transformation_name)] = [cityblock(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['jaccard_distance_{}'.format(transformation_name)] = [jaccard(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['canberra_distance_{}'.format(transformation_name)] = [canberra(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['euclidean_distance_{}'.format(transformation_name)] = [euclidean(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['minkowski_distance_{}'.format(transformation_name)] = [minkowski(x, y, 3) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "                                                              np.nan_to_num(question2_vectors))]\n",
    "\n",
    "    data['braycurtis_distance_{}'.format(transformation_name)] = [braycurtis(x, y) for (x, y) in zip(np.nan_to_num(question1_vectors),\n",
    "    np.nan_to_num(question2_vectors))]\n",
    "    return data\n",
    "\n",
    "def SVD_text_tfidf(df2, ndims, gram_range, analyze = 'word'):\n",
    "    df = df2.copy()\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999, ngram_range = (1, gram_range),\n",
    "                        analyzer = analyze)\n",
    "    svd = TruncatedSVD(ndims, random_state = seed)\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    tf.fit(df.question1 + df.question2)\n",
    "    bow = tf.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = tf.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        svds.append(svd_i)\n",
    "    return svds\n",
    "\n",
    "def LSA_text_tfidf(df2, ndims, gram_range, analyze = 'word'):\n",
    "    tf = TfidfVectorizer(stop_words = 'english', min_df = 1, max_df = 0.999, ngram_range = (1, gram_range),\n",
    "                        analyzer = analyze)\n",
    "    svd = TruncatedSVD(ndims, random_state = 1337)\n",
    "    normalizer = Normalizer(copy = False)\n",
    "    df = df2.copy()\n",
    "    text_feats = df.select_dtypes(include=['object']).columns.values\n",
    "    tf.fit(df.question1 + df.question2)\n",
    "    bow = tf.transform(df.question1 + df.question2)\n",
    "    svd.fit(bow)\n",
    "    transformed_bow = svd.transform(bow)\n",
    "    normed_bow = normalizer.fit(transformed_bow)\n",
    "    svds = []\n",
    "    for i in text_feats:\n",
    "        bow_i = tf.transform(df[i])\n",
    "        svd_i = svd.transform(bow_i)\n",
    "        normed_i = normalizer.transform(svd_i)\n",
    "        svds.append(normed_i)\n",
    "    return svds\n",
    "\n",
    "def run_transforms(transformation_name, ndims, gram_range, analyze, test = False):\n",
    "    lsa_dff = LSA_text_tfidf(df_full, ndims, gram_range, analyze)\n",
    "    svd_dff = SVD_text_tfidf(df_full, ndims, gram_range, analyze)\n",
    "    lsa_tr, lsa_te = split_traintest(lsa_dff)\n",
    "    svd_tr, svd_te = split_traintest(svd_dff)\n",
    "    tr_lsa_dist = get_distances('train_LSA_{}'.format(transformation_name), lsa_tr[0], lsa_tr[1])\n",
    "    tr_svd_dist = get_distances('train_SVD_{}'.format(transformation_name), svd_tr[0], svd_tr[1])\n",
    "    tr_lsa_dist.to_csv('train_LSA_{}.csv'.format(transformation_name), index = False)\n",
    "    tr_svd_dist.to_csv('train_SVD_{}'.format(transformation_name), index = False)\n",
    "    if test:\n",
    "        te_lsa_dist = get_distances('test_LSA_{}'.format(transformation_name), lsa_te[0], lsa_te[1])\n",
    "        te_svd_dist = get_distances('test_SVD_{}'.format(transformation_name), svd_te[0], svd_te[1])\n",
    "        te_lsa_dist.to_csv('test_LSA_{}.csv'.format(transformation_name), index = False)\n",
    "        te_svd_dist.to_csv('test_SVD_{}'.format(transformation_name), index = False)\n",
    "    return\n",
    "\n",
    "run_transforms('TFIDF_3grams_words_50dim', 50, 3, 'word', test = True)\n",
    "run_transforms('TFIDF_5grams_words_50dim', 50, 5, 'word', test = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
